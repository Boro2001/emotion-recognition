{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('train.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: extra compressed data\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "Corrupt JPEG data: 50 extraneous bytes before marker 0xd9\n"
     ]
    }
   ],
   "source": [
    "# image prepearation for dominace\n",
    "import cv2\n",
    "import os\n",
    "X = []\n",
    "y = []\n",
    "for i, row in data.iterrows():\n",
    "    if i > 10000: # only use first 10000 images\n",
    "        break\n",
    "    try:\n",
    "        x1, y1, x2, y2 = row['bbox'][0], row['bbox'][1], row['bbox'][2], row['bbox'][3]\n",
    "        img = cv2.imread(row['paths'])[y1:y2, x1:x2] # crop image\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
    "        img = cv2.resize(img, (224, 224)) # resize to 224x224\n",
    "        img = img / 255.0 # normalize\n",
    "        img = np.array(img) \n",
    "        X.append(img)\n",
    "        y.append(row['dominance'])\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
    "def get_run_logdir(param_str):    \n",
    "    run_id = str(int(time.time())) + param_str\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 16:19:57.270315: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223/223 [==============================] - ETA: 0s - loss: 1.8776 - accuracy: 0.2809"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 16:22:01.418642: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223/223 [==============================] - 130s 575ms/step - loss: 1.8776 - accuracy: 0.2809 - val_loss: 1.8016 - val_accuracy: 0.3363\n",
      "Epoch 2/5\n",
      "223/223 [==============================] - 127s 569ms/step - loss: 1.8080 - accuracy: 0.2963 - val_loss: 1.7650 - val_accuracy: 0.3363\n",
      "Epoch 3/5\n",
      "223/223 [==============================] - 126s 567ms/step - loss: 1.7844 - accuracy: 0.3021 - val_loss: 1.7553 - val_accuracy: 0.3363\n",
      "Epoch 4/5\n",
      "223/223 [==============================] - 127s 568ms/step - loss: 1.7772 - accuracy: 0.3031 - val_loss: 1.7474 - val_accuracy: 0.3363\n",
      "Epoch 5/5\n",
      "223/223 [==============================] - 126s 567ms/step - loss: 1.7858 - accuracy: 0.3039 - val_loss: 1.7864 - val_accuracy: 0.3363\n",
      "62/62 [==============================] - 13s 213ms/step - loss: 1.7733 - accuracy: 0.3114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 16:30:47.442906: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# simle CNN model\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "from functools import partial\n",
    "DefaultConv2D = partial(keras.layers.Conv2D,\n",
    "                        kernel_size=2, activation='relu', padding=\"SAME\")\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape = (224,224,1)),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])\n",
    "\n",
    "run_logdir = get_run_logdir(\"_simpleCNN_\" + \"dominance\")\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "es = tf.keras.callbacks.EarlyStopping(min_delta=1.0, patience=10, verbose=1)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_split=0.1, callbacks=[tensorboard_cb, es])\n",
    "\n",
    "score = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:10] \n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bbox</th>\n",
       "      <th>paths</th>\n",
       "      <th>discrete_emotion</th>\n",
       "      <th>valance</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "      <th>first_emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[86, 58, 564, 628]</td>\n",
       "      <td>./emotic/mscoco/images/COCO_val2014_0000005622...</td>\n",
       "      <td>[Disconnection, Doubt/Confusion]</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[485, 149, 605, 473]</td>\n",
       "      <td>./emotic/mscoco/images/COCO_train2014_00000028...</td>\n",
       "      <td>[Anticipation]</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[305, 92, 461, 465]</td>\n",
       "      <td>./emotic/mscoco/images/COCO_val2014_0000005581...</td>\n",
       "      <td>[Engagement, Excitement, Happiness]</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[221, 63, 448, 372]</td>\n",
       "      <td>./emotic/mscoco/images/COCO_train2014_00000036...</td>\n",
       "      <td>[Aversion, Pleasure]</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[44, 143, 150, 288]</td>\n",
       "      <td>./emotic/mscoco/images/COCO_train2014_00000021...</td>\n",
       "      <td>[Confidence, Excitement]</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17072</th>\n",
       "      <td>[189, 194, 323, 438]</td>\n",
       "      <td>./emotic/mscoco/images/COCO_val2014_0000002037...</td>\n",
       "      <td>[Anticipation, Engagement]</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17073</th>\n",
       "      <td>[214, 48, 340, 326]</td>\n",
       "      <td>./emotic/mscoco/images/COCO_train2014_00000017...</td>\n",
       "      <td>[Confidence]</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17074</th>\n",
       "      <td>[166, 35, 341, 401]</td>\n",
       "      <td>./emotic/mscoco/images/COCO_val2014_0000005140...</td>\n",
       "      <td>[Anticipation, Engagement, Excitement]</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17075</th>\n",
       "      <td>[245, 227, 293, 340]</td>\n",
       "      <td>./emotic/framesdb/images/frame_k7fb824vh221kl3...</td>\n",
       "      <td>[Engagement]</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17076</th>\n",
       "      <td>[189, 169, 314, 410]</td>\n",
       "      <td>./emotic/mscoco/images/COCO_val2014_0000002446...</td>\n",
       "      <td>[Sympathy]</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16896 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       bbox  \\\n",
       "0        [86, 58, 564, 628]   \n",
       "1      [485, 149, 605, 473]   \n",
       "2       [305, 92, 461, 465]   \n",
       "3       [221, 63, 448, 372]   \n",
       "4       [44, 143, 150, 288]   \n",
       "...                     ...   \n",
       "17072  [189, 194, 323, 438]   \n",
       "17073   [214, 48, 340, 326]   \n",
       "17074   [166, 35, 341, 401]   \n",
       "17075  [245, 227, 293, 340]   \n",
       "17076  [189, 169, 314, 410]   \n",
       "\n",
       "                                                   paths  \\\n",
       "0      ./emotic/mscoco/images/COCO_val2014_0000005622...   \n",
       "1      ./emotic/mscoco/images/COCO_train2014_00000028...   \n",
       "2      ./emotic/mscoco/images/COCO_val2014_0000005581...   \n",
       "3      ./emotic/mscoco/images/COCO_train2014_00000036...   \n",
       "4      ./emotic/mscoco/images/COCO_train2014_00000021...   \n",
       "...                                                  ...   \n",
       "17072  ./emotic/mscoco/images/COCO_val2014_0000002037...   \n",
       "17073  ./emotic/mscoco/images/COCO_train2014_00000017...   \n",
       "17074  ./emotic/mscoco/images/COCO_val2014_0000005140...   \n",
       "17075  ./emotic/framesdb/images/frame_k7fb824vh221kl3...   \n",
       "17076  ./emotic/mscoco/images/COCO_val2014_0000002446...   \n",
       "\n",
       "                             discrete_emotion  valance  arousal  dominance  \\\n",
       "0            [Disconnection, Doubt/Confusion]        5        3          9   \n",
       "1                              [Anticipation]        6        4          7   \n",
       "2         [Engagement, Excitement, Happiness]        7        8          8   \n",
       "3                        [Aversion, Pleasure]        8        9          8   \n",
       "4                    [Confidence, Excitement]        7        9         10   \n",
       "...                                       ...      ...      ...        ...   \n",
       "17072              [Anticipation, Engagement]        6        5          3   \n",
       "17073                            [Confidence]        7        8          7   \n",
       "17074  [Anticipation, Engagement, Excitement]        6        2         10   \n",
       "17075                            [Engagement]        5        5          6   \n",
       "17076                              [Sympathy]        6        4          8   \n",
       "\n",
       "       first_emotion  \n",
       "0                 12  \n",
       "1                  3  \n",
       "2                  4  \n",
       "3                 17  \n",
       "4                  5  \n",
       "...              ...  \n",
       "17072              3  \n",
       "17073              5  \n",
       "17074              3  \n",
       "17075              4  \n",
       "17076             10  \n",
       "\n",
       "[16896 rows x 7 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "3\n",
      "4\n",
      "17\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "13\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "X = []\n",
    "y = []\n",
    "for i, row in data.iterrows():\n",
    "    if i > 10: # only use first 10000 images\n",
    "        break\n",
    "    try:\n",
    "        x1, y1, x2, y2 = row['bbox'][0], row['bbox'][1], row['bbox'][2], row['bbox'][3]\n",
    "        img = cv2.imread(row['paths'])[y1:y2, x1:x2] # crop image\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
    "        img = cv2.resize(img, (224, 224)) # resize to 224x224\n",
    "        img = img / 255.0 # normalize\n",
    "        img = np.array(img) \n",
    "        X.append(img)\n",
    "        y.append(row['first_emotion'])\n",
    "        print(row['first_emotion'])\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%tensorboard --logdir tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The input must have 3 channels; Received `input_shape=(224, 224, 1)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/mikolajborowicz/Desktop/emotion-recognition/neural-network-training.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mikolajborowicz/Desktop/emotion-recognition/neural-network-training.ipynb#ch0000008?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m img_to_array\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mikolajborowicz/Desktop/emotion-recognition/neural-network-training.ipynb#ch0000008?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m array_to_img\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mikolajborowicz/Desktop/emotion-recognition/neural-network-training.ipynb#ch0000008?line=14'>15</a>\u001b[0m model \u001b[39m=\u001b[39m Xception(weights\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mimagenet\u001b[39;49m\u001b[39m'\u001b[39;49m, include_top\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, input_shape\u001b[39m=\u001b[39;49m(\u001b[39m224\u001b[39;49m,\u001b[39m224\u001b[39;49m,\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/applications/xception.py:127\u001b[0m, in \u001b[0;36mXception\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mIf using `weights` as `\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m` with `include_top`\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    124\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39m as true, `classes` should be 1000\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[39m# Determine proper input shape\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m input_shape \u001b[39m=\u001b[39m imagenet_utils\u001b[39m.\u001b[39;49mobtain_input_shape(\n\u001b[1;32m    128\u001b[0m     input_shape,\n\u001b[1;32m    129\u001b[0m     default_size\u001b[39m=\u001b[39;49m\u001b[39m299\u001b[39;49m,\n\u001b[1;32m    130\u001b[0m     min_size\u001b[39m=\u001b[39;49m\u001b[39m71\u001b[39;49m,\n\u001b[1;32m    131\u001b[0m     data_format\u001b[39m=\u001b[39;49mbackend\u001b[39m.\u001b[39;49mimage_data_format(),\n\u001b[1;32m    132\u001b[0m     require_flatten\u001b[39m=\u001b[39;49minclude_top,\n\u001b[1;32m    133\u001b[0m     weights\u001b[39m=\u001b[39;49mweights)\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m input_tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m   img_input \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39minput_shape)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/applications/imagenet_utils.py:372\u001b[0m, in \u001b[0;36mobtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    370\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`input_shape` must be a tuple of three integers.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    371\u001b[0m \u001b[39mif\u001b[39;00m input_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m weights \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 372\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe input must have 3 channels; Received \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    373\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m`input_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m ((input_shape[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m input_shape[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m min_size) \u001b[39mor\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     (input_shape[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m input_shape[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m min_size)):\n\u001b[1;32m    376\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInput size must be at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    377\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmin_size\u001b[39m}\u001b[39;00m\u001b[39mx\u001b[39m\u001b[39m{\u001b[39;00mmin_size\u001b[39m}\u001b[39;00m\u001b[39m; Received: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    378\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39minput_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The input must have 3 channels; Received `input_shape=(224, 224, 1)`"
     ]
    }
   ],
   "source": [
    "# training xception model with keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "\n",
    "model = Xception(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62a727df985e3daf18bc9d1942c89ef5df65520a7e91f46a7d19652d0d54c774"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
